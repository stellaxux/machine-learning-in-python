{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement K-nearest neighbors algorithm (KNN)\n",
    "\n",
    "KNN is a nonparametric model for classification which doesn't learn a discriminative function from the training data but memorizes the training dataset instead. The model for kNN is the entire training dataset. When a prediction is required for a unseen data instance, the kNN algorithm will search through the training dataset for the k-most similar instances. The similarity measure is dependent on the type of data. For real-valued data, the Euclidean distance is usually used. Other other types of data such as categorical or binary data, Hamming distance can be used. If we are using a Euclidean distance measure, it is also important to standardize the data so that each feature contributes equally to the distance. In the case of regression problems, the average of the predicted attribute is returned. In classification, the most prevalent class is returned.\n",
    "\n",
    "The main advantage of KNN is that the classifier immediately adapts as we collect new training data. A disadvantage of KNN is that it can be computationally expensive to repeat the same or similar searches over larger training datasets. It is very susceptible to overfitting due to the curse of dimensionality. In addition, storage space can become a challenge if we are working with large datasets.\n",
    "\n",
    "The algorithm itself is straightforward and can be summarized by the following steps:\n",
    "    1. Choose the number of k and a distance metric.\n",
    "    2. Find the k nearest neighbors of the sample that we want to classify.\n",
    "    3. Assign the class label by majority vote.\n",
    "\n",
    "In this exercise, I will implement the KNN algorithm from scratch. The implementation will be will be demonstrated using the Iris flowers classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7320508075688772"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclideanDist(x1, x2):\n",
    "    # input x1 and x2 must be numpy arrays\n",
    "    return np.sqrt(np.sum((x1-x2)**2))\n",
    "\n",
    "# test the function\n",
    "x1 = np.array([0, 0, 0])\n",
    "x2 = np.array([1, 1, 1])\n",
    "dist = euclideanDist(x1, x2)\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement knn algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def knn(X_train, y_train, x, k):\n",
    "    \"\"\"calculating k nearest neighbors of a new point x, predict its label based on majority vote\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : array-like, shape = [n_samples, n_features]\n",
    "        Training vectors, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "    y_train : array-like, shape = [n_samples]\n",
    "        Target label\n",
    "    x : 1-D array, shape = [1, n_feature]\n",
    "        New point whose label is being assigned\n",
    "    k : int, the number of neighbors used to determine the label of x\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y : Target label for the new point x\n",
    "\n",
    "    \"\"\"\n",
    "    # calculate distance between x and each sample in training data\n",
    "    dists = np.array([euclideanDist(t, x) for t in X_train])\n",
    "    # sort the dists array to find the index of k nearest samples\n",
    "    sortedDist = np.argsort(dists)\n",
    "    # count the number of vote for each label existing in the k samples using dictionary\n",
    "    classVotes = {}\n",
    "    for i in range(k):\n",
    "        label = y_train[sortedDist[i]]\n",
    "        classVotes[label] = classVotes.get(label, 0) + 1\n",
    "        # the above line is equall to\n",
    "        # if label in classVotes:\n",
    "        #    classVotes[label] += 1\n",
    "        # else:\n",
    "            #classVotes[label] = 1\n",
    "    # sort the dictionary classVotes by value and return the major vote\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the knn algorithm with iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred = 2\n",
      "y_true = 2\n"
     ]
    }
   ],
   "source": [
    "# Load iris dataset from scikit-learn. \n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Splitting data into 70% training and 30% test data:\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Test out with a single point in test set\n",
    "y_pred = knn(X_train, y_train, X_test[0], k=2)\n",
    "print ('y_pred = %s' % y_pred)\n",
    "print ('y_true = %s' % y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.78%\n"
     ]
    }
   ],
   "source": [
    "# Apply the knn algorithm to predict the labels of samples in the testset\n",
    "y_pred = []\n",
    "for x in X_test:\n",
    "    y = knn(X_train, y_train, x, k=2)\n",
    "    y_pred.append(y)\n",
    "\n",
    "# write a function to calculate classification accuracy of prediction\n",
    "def getAccuracy(y_pred, y_true):\n",
    "    return np.sum([y_pred == y_true])/len(y_true)\n",
    "\n",
    "# calculate classification accuracy on the testset\n",
    "y_pred = np.array(y_pred)\n",
    "accuracy = getAccuracy(y_pred, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ideas For Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression: The knn algorithm can be adapted to work for regression problems (predicting a real-valued attribute). The summarization of the closest instances could involve taking the mean or the median of the predicted attribute\n",
    "\n",
    "- Normalization: When the units of measure differ between attributes, it is possible for attributes to dominate in their contribution to the distance measure. For these types of problems, we need to rescale all data attributes into the range 0-1 (called normalization) before calculating similarity\n",
    "\n",
    "- Alternative Distance Measure: There are many distance measures available. The code can be modify to implement an alternative distance measure, such as Manhattan distance or the vector dot product\n",
    "\n",
    "- Support for distance-weighted contribution for the k-most similar instances to the prediction\n",
    "- More advanced data tree-based structures for searching for similar instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sample code for standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python3]",
   "language": "python",
   "name": "Python [python3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
